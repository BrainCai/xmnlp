{
  "name": "Xmnlp",
  "tagline": "xmnlp中文分词工具，java编写，统计概率分词+规则分词实现，功能包括人名识别，词性标注，用户自定义词典扩展，分词效果速度都超过开源版的jieba分词。 ",
  "body": "# xmnlp自然语言处理包\r\n\r\n\r\n#### author：xuming(shibing624)\r\n#### environment：jdk 1.6\r\n\r\n\r\n1. add the rule chinese word segmentation: 2016.06.21\r\n    - 正向最大匹配法\r\n    - 逆向最大匹配法\r\n    - 双向最大匹配法\r\n2. add jieba segmentation java 2016.07.06\r\n\t- 基于 trie 树结构实现高效词图扫描\r\n  \t- 生成所有切词可能的有向无环图 DAG\r\n  \t- 采用动态规划算法计算最佳切词组合\r\n  \t- 基于 =HMM= 模型，采用 =Viterbi= (维特比)算法实现未登录词识别\r\n3. add xmnlp to xmnlp 2016.07.23\r\n\t- 基于双数组 trie 树结构实现高效词图扫描存储词典\r\n\t- 最短路径分词，最短路求解采用Viterbi算法\r\n\t- 添加人名识别功能：中文人名，日本人名，音译人名识别功能\r\n4. alter jdk from 1.7 to 1.6 2016.07.24\r\n\t- 兼容jdk1.6版本，把所有用jdk1.7甚至1.8的写法用1.6重写了\r\n5. add organization recognition, place recognization ,ahocorasick algoritm and tire tree 20160724 xuming\r\n\t- 机构名识别功能\r\n\t- 地名识别\r\n\t- trie词典树，基于贝尔实验室的 Aho-Corasick 白皮书完成，将来可以改为双数组trie数提高效率\r\n6. add pinyin, keyword, stopword, POS tagging, synonym dictionary and traditionary chinese to xmnlp 2016.07.24 xuming\r\n\t- 汉字转拼音功能：结果可以显示为数字音调，符号音调，无音调，声调，声母，韵母，输入法头\r\n\t- 提取关键字\r\n\t- 添加搜索停用词\r\n\t- 词性标注\r\n\t- 同义词典\r\n\t- 简繁字体转换\r\n7. add high speed segment, HMM segment, index segment xuming 20160730\r\n\t- 急速分词器，基于双数组前缀树词典结构，查找词典的最大匹配分词\r\n\t- 二阶隐马模型分词器\r\n\t- 索引分词器\r\n8. add crf segment , mutithread segment, nshort segment and all test  xuming 20160730\r\n\t- 条件随机场模型分词器\r\n\t- 多线程处理，配合CRF分词效果和速度有保障\r\n\t- NShort分词器\r\n\t- 全覆盖单元测试\r\n9. add dependency parse, include 2-gram dependency model, CRF model dependency, Max Ent model dependency and all test  xuming 20160731\r\n\t- 依存句法分析工具\r\n\t- 2-gram依存模型，根据两个词的词和词性猜测它们最可能的依存关系\r\n\t- 条件随机场（CRF）句法模型\r\n\t- 最大熵（MaxEnt）句法分析器\r\n\t- 神经网络句法模型，待完善\r\n10. add web demo xuming 20160801\r\n\t- 几种中文分词方式演示页面：精准分词方式，NLP分词，CRF、HMM分词（需要设置jvm：-Xmx1024M）\r\n\t- 关键词提取演示\r\n\t- 拼音标注\r\n\t- 繁体句子分词\r\n\t- 繁体字转简体\r\n\t- 句法分析\r\n\t- 文本推荐演示\r\n11. add rule segment:forwardMinSeg,reverseMinSeg,biMinSeg,biMaxMinSeg,MaxNgram. xuming 20160830\r\n    - 正向最小分词\r\n    - 反向最小分词\r\n    - 双向最小分词\r\n    - 双向最大最小分词\r\n    - 最大Ngram分值算法分词\r\n12. add rule ambiguity test and jieba ambiguity test. xuming 20160908\r\n    - 规则分词（正向、反向、双向匹配算法等）歧义性测试（效果不理想）\r\n    - 结巴分词歧义性测试（效果不理想）\r\n13. add ansj segmentation. xuming 20160928\r\n    - ansj中文分词器\r\n---\r\n\r\n**xmnlp**是由一系列模型与算法组成的Java工具包，目标是普及自然语言处理在生产环境中的应用。**xmnlp**具备功能完善、性能高效、架构清晰、语料时新、可自定义的特点。\r\n\r\n**xmnlp**提供下列功能：\r\n\r\n> * 中文分词.\r\n  * 最短路分词.\r\n  * N-最短路分词\r\n  * CRF分词.\r\n  * 索引分词.\r\n  * 极速词典分词.\r\n  * 用户自定义词典.\r\n> * 词性标注.\r\n> * 命名实体识别.\r\n  * 中国人名识别.\r\n  * 音译人名识别.\r\n  * 日本人名识别.\r\n  * 地名识别.\r\n  * 实体机构名识别.\r\n> * 关键词提取.\r\n  * TextRank关键词提取.\r\n> * 自动摘要.\r\n  * TextRank自动摘要.\r\n> * 短语提取.\r\n  * 基于互信息和左右信息熵的短语提取.\r\n> * 拼音转换.\r\n  * 多音字.\r\n  * 声母.\r\n  * 韵母.\r\n  * 声调.\r\n> * 简繁转换.\r\n  * 繁体中文分词.\r\n  * 简繁分歧词.\r\n> * 文本推荐.\r\n  * 语义推荐.\r\n  * 拼音推荐.\r\n  * 字词推荐.\r\n> * 依存句法分析.\r\n  * 基于神经网络的高性能依存句法分析器.\r\n  * MaxEnt依存句法分析.\r\n  * CRF依存句法分析.\r\n> * 语料库工具.\r\n  * 分词语料预处理.\r\n  * 词频词性词典制作.\r\n  * BiGram统计.\r\n  * 词共现统计.\r\n  * CoNLL语料预处理.\r\n\r\n\r\n在提供丰富功能的同时，**xmnlp**内部模块坚持低耦合、模型坚持惰性加载、服务坚持静态提供、词典坚持明文发布，使用非常方便，同时自带一些语料处理工具，帮助用户训练自己的语料。\r\n\r\n------\r\n\r\n\r\n\r\n## 如何获取\r\n  - 方式一：当前稳定版本\r\n\r\n    ```\r\n  \t<dependency>\r\n            <groupId>com.xm</groupId>\r\n            <artifactId>xmnlp</artifactId>\r\n            <version>1.0.1</version>\r\n    </dependency>\r\n  \t```\r\n\r\n    零配置，即可使用基本功能（除CRF分词、依存句法分析外的全部功能）。如果用户有自定义的需求，可以参考方式二，使用xmnlp.properties进行配置。\r\n\r\n  - 方式二：下载jar、data、xmnlp.properties\r\n\r\n  \t数据与程序分离，给予用户自定义的自由。\r\n\r\n  \t1、下载jar\r\n\r\n  \t  xmnlp.jar\r\n\r\n  \t2、下载data\r\n\r\n\t  [data.zip](http://pan.baidu.com/s/1jIydVsq)\r\n\t  下载后解压到任意目录，接下来通过配置文件告诉xmnlp数据包的位置。\r\n\r\n\t  xmnlp中的数据分为词典和模型，其中词典是词法分析必需的，模型是句法分析必需的。\r\n\r\n\r\n\t>\r\n\t> \t  data\r\n\t> \t  │\r\n\t> \t  ├─dictionary\r\n\t> \t  └─model\r\n\r\n\t  用户可以自行增删替换，如果不需要句法分析功能的话，随时可以删除model文件夹。\r\n\r\n\t  模型跟词典没有绝对的区别，隐马模型被做成人人都可以编辑的词典形式，不代表它不是模型。\r\n\t  GitHub代码库中已经包含了data.zip中的词典，直接编译运行自动缓存即可；模型则需要额外下载。\r\n\r\n  \t3、配置文件\r\n\r\n\t  示例配置文件:`xmnlp.properties` 在src的resources中。\r\n\r\n\t  配置文件的作用是告诉xmnlp数据包的位置，只需修改第一行\r\n\r\n\t  root=usr/home/xmnlp/\r\n\t  为data的父目录即可，比如data目录是/Users/root/Documents/data，那么root=/Users/root/Documents/ 。\r\n\r\n\t  如果选用mini词典的话，则需要修改配置文件：\r\n\r\n\t  CoreDictionaryPath=data/dictionary/CoreNatureDictionary.mini.txt\r\n\t  BiGramDictionaryPath=data/dictionary/CoreNatureDictionary.ngram.mini.txt\r\n\r\n\t  最后将xmnlp.properties放入classpath即可，对于任何项目，都可以放到src或resources目录下，编译时IDE会自动将其复制到classpath中。\r\n\r\n\t  如果放置不当，xmnlp会智能提示当前环境下的合适路径，并且尝试从项目根目录读取数据集。\r\n\r\n## 调用方法\r\n\r\n**xmnlp**几乎所有的功能都可以通过工具类`Xmnlp`快捷调用，当你想不起来调用方法时，只需键入`Xmnlp.`，IDE应当会给出提示，并展示**xmnlp**完善的文档。\r\n\r\n*推荐用户始终通过工具类`Xmnlp`调用，这么做的好处是，将来**xmnlp**升级后，用户无需修改调用代码。*\r\n\r\n所有Demo都位于[org.xm.xmnlp.demo](https://github.com/shibing624/xmnlp/tree/master/src/test/java/org/xm/xmnlp/demo)下，比文档覆盖了更多细节，强烈建议运行一遍。\r\n\r\n#### 如何使用\r\n\r\n  - Demo\r\n\r\n\r\n\t```\r\n\tSystem.out.println(Xmnlp.segment(\"你好，欢迎使用xmnlp自然语言处理包！\"));\r\n\t```\r\n\r\n## 支持结巴中文分词模式\r\n   - Search模式，用于对用户查询词分词\r\n   - Index模式，用于对索引文档分词\r\n\r\n#### 特性\r\n   - 支持多种分词模式\r\n   - 全角统一转成半角\r\n   - 用户词典功能\r\n   - resource 目录有整理的搜狗细胞词库和一个自定义词库，可加载多个用户词库\r\n\r\n#### 算法\r\n  - [ ] 基于 =trie= 树结构实现高效词图扫描\r\n  - [ ] 生成所有切词可能的有向无环图 =DAG=\r\n  - [ ] 采用动态规划算法计算最佳切词组合\r\n  - [ ] 基于 =HMM= 模型，采用 =Viterbi= (维特比)算法实现未登录词识别\r\n\r\n## 许可证\r\n  许可证为ApacheLicence 2.0\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}